{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Under development: 2019 Welcome to plant genomics training Click on the topics in the left hand menu or use the search bar in the top right corner. This website provides training for plant genomics. It has been designed for the Australian plant genomics community as part of the Genomics for Australian Plants project, by Bioplatforms Australia. https://www.genomicsforaustralianplants.com/","title":"Home"},{"location":"#welcome-to-plant-genomics-training","text":"Click on the topics in the left hand menu or use the search bar in the top right corner. This website provides training for plant genomics. It has been designed for the Australian plant genomics community as part of the Genomics for Australian Plants project, by Bioplatforms Australia. https://www.genomicsforaustralianplants.com/","title":"Welcome to plant genomics training"},{"location":"modules/computing/","text":"Unix Software carpentry tutorial: http://swcarpentry.github.io/shell-novice/ Guide to Unix: https://f1000research.com/documents/7-1436 Python Software carpentry tutorial: http://swcarpentry.github.io/python-novice-gapminder/ Biopython: https://biopython.org/ Solving biological problems with python: http://pycam.github.io/ R Software carpentry tutorial: http://swcarpentry.github.io/r-novice-inflammation/ Git Software carpentry tutorial: http://swcarpentry.github.io/git-novice/ Algorithms https://www.coursera.org/learn/dna-sequencing https://www.youtube.com/user/BenLangmead Containers https://github.com/pawseysc/bio-workshop-18 https://www.melbournebioinformatics.org.au/tutorials/tutorials/docker/docker/","title":"Computing"},{"location":"modules/computing/#unix","text":"Software carpentry tutorial: http://swcarpentry.github.io/shell-novice/ Guide to Unix: https://f1000research.com/documents/7-1436","title":"Unix"},{"location":"modules/computing/#python","text":"Software carpentry tutorial: http://swcarpentry.github.io/python-novice-gapminder/ Biopython: https://biopython.org/ Solving biological problems with python: http://pycam.github.io/","title":"Python"},{"location":"modules/computing/#r","text":"Software carpentry tutorial: http://swcarpentry.github.io/r-novice-inflammation/","title":"R"},{"location":"modules/computing/#git","text":"Software carpentry tutorial: http://swcarpentry.github.io/git-novice/","title":"Git"},{"location":"modules/computing/#algorithms","text":"https://www.coursera.org/learn/dna-sequencing https://www.youtube.com/user/BenLangmead","title":"Algorithms"},{"location":"modules/computing/#containers","text":"https://github.com/pawseysc/bio-workshop-18 https://www.melbournebioinformatics.org.au/tutorials/tutorials/docker/docker/","title":"Containers"},{"location":"modules/data/","text":"How to get data Three common options are wget, curl, and sra-toolkit. wget [address of file] curl [address of file] Find some SRR numbers for reads you are interested in (search NCBI\u2019s SRA ), then: fastq-dump SRR[number goes here] More information: https://ncbi.github.io/sra-tools/install_config.html https://isugenomics.github.io/bioinformatics-workbook/dataAcquisition/fileTransfer/sra.html Note: the SRR numbers refer to the sequencing run. https://www.ncbi.nlm.nih.gov/sra/docs/submitmeta/ Sample sequencing data PacBio reads From Canu\u2019s quickstart page: curl -L -o pacbio.fastq http://gembox.cbcb.umd.edu/mhap/raw/ecoli_p6_25x.filtered.fastq Synthetic PacBio data wget https://zenodo.org/record/1009308/files/pacbio.fq?download=1 Nanopore reads From Canu\u2019s quickstart page; data from the Loman lab curl -L -o oxford.fasta http://nanopore.s3.climb.ac.uk/MAP006-PCR-1_2D_pass.fasta Downsampled nanopore data wget https://zenodo.org/record/842795/files/minion_1.2d_pass.fastq.fastqsanger?download=1 fast5 files https://figshare.com/articles/Raw_ONT_reads_-_barcode_1/5353210 10X (linked) reads https://www.10xgenomics.com/resources/datasets/ Mixed: Training: long read workshop in sydney - also contains data https://zenodo.org/record/1442930#.XHXRLlMza-4","title":"Data"},{"location":"modules/data/#how-to-get-data","text":"Three common options are wget, curl, and sra-toolkit. wget [address of file] curl [address of file] Find some SRR numbers for reads you are interested in (search NCBI\u2019s SRA ), then: fastq-dump SRR[number goes here] More information: https://ncbi.github.io/sra-tools/install_config.html https://isugenomics.github.io/bioinformatics-workbook/dataAcquisition/fileTransfer/sra.html Note: the SRR numbers refer to the sequencing run. https://www.ncbi.nlm.nih.gov/sra/docs/submitmeta/","title":"How to get data"},{"location":"modules/data/#sample-sequencing-data","text":"PacBio reads From Canu\u2019s quickstart page: curl -L -o pacbio.fastq http://gembox.cbcb.umd.edu/mhap/raw/ecoli_p6_25x.filtered.fastq Synthetic PacBio data wget https://zenodo.org/record/1009308/files/pacbio.fq?download=1 Nanopore reads From Canu\u2019s quickstart page; data from the Loman lab curl -L -o oxford.fasta http://nanopore.s3.climb.ac.uk/MAP006-PCR-1_2D_pass.fasta Downsampled nanopore data wget https://zenodo.org/record/842795/files/minion_1.2d_pass.fastq.fastqsanger?download=1 fast5 files https://figshare.com/articles/Raw_ONT_reads_-_barcode_1/5353210 10X (linked) reads https://www.10xgenomics.com/resources/datasets/ Mixed: Training: long read workshop in sydney - also contains data https://zenodo.org/record/1442930#.XHXRLlMza-4","title":"Sample sequencing data"},{"location":"modules/docker/","text":"Containers Containers are a collection of tools and their dependencies, packaged into a file. You can download and run this file and its software, without directly installing the software itself. Types of containers: Docker, Singularity The basics Download and install Docker Desktop: https://www.docker.com/products/docker-desktop Open docker (e.g.: if it\u2019s running, it may show an icon in the top menu bar). In the terminal (or equivalent): docker pull busybox //pulls the image named \"busybox\" from DockerHub docker run busybox //creates the container, then exits if no tasks are running docker run busybox ls //creates container, lists files, exits docker image ls //lists all docker images docker --help // docker help Getting files in and out of containers We will make a folder that can get share files with our containers. In your home directory (/Users/yourname/): mkdir workspace cd workspace docker run -v /Users/[yourname]/workspace:/data/ busybox touch data/plantfile -v connectes our local \u201cworkspace\u201d folder with a \u201cdata\u201d folder in the container. Around this, we docker run busybox Finally, we create a file with touch data/plantfile Example: assemble and annotate a bacterial genome Get some sample data files (synthetic bacterial illumina reads): wget https://zenodo.org/record/582600/files/mutant_R1.fastq?download=1 wget https://zenodo.org/record/582600/files/mutant_R2.fastq?download=1 Shorten the names: mv mutant_R1.fastq?download=1 mutant_R1.fastq mv mutant_R2.fastq?download=1 mutant_R2.fastq Run the assembly tool (spades) in a container: docker run -v /Users/[yourname]/workspace:/data/ quay.io/biocontainers/spades:3.13.0--0 ./usr/local/bin/spades.py -1 data/mutant_R1.fastq -2 data/mutant_R2.fastq -o data/spades_outdir quay.io/biocontainers/spades:3.13.0\u20130 is the image name ./usr/local/bin/spades.py is the command to run spades -1 specifies the file with forward reads, data/mutant_R1.fastq -2 specifies the file with reverse reads, data/mutant_R2.fastq -o specifies the output directory, data/spades_outdir ls //to see the output files in your local workspace folder Move the assembled reads - the contigs file - into the workspace folder: mv spades_outdir/contigs.fasta . Run the annotation tool (prokka) in a container: docker run -v /Users/[yourname]/workspace:/data/ ummidock/prokka:1.13.3-01 prokka contigs.fasta ummidock/prokka:1.13.3-01 is the image name prokka is the command to run prokka the input file is contigs.fasta this container has been configured to start within the data folder, so \u201cdata/\u201d is not needed before the input file this container has also been configured to put output in the data folder, so it is not necessary to specify an output directory name ls //to see the annotation output cd PROKKA_outfile_name less PROKKA_outfile.txt //summary of annotation Troubleshooting How do we know what the command is to run the tool? Run the container interactively: e.g. docker run -it ummidock/prokka:1.13.3-01 Try the tool name and the help flag: e.g. prokka -h How do we know if the container has a data folder? Run the container, print the working directory: e.g. docker run ummidock/prokka:1.13.3-01 pwd If the working directory is called \u201cworkdir\u201d, use that in place of \u201cdata\u201d e.g. -v /Users/[yourname]/workspace:/workdir/ Links Docker: https://docs.docker.com/get-started/","title":"Using containers"},{"location":"modules/docker/#containers","text":"Containers are a collection of tools and their dependencies, packaged into a file. You can download and run this file and its software, without directly installing the software itself. Types of containers: Docker, Singularity","title":"Containers"},{"location":"modules/docker/#the-basics","text":"Download and install Docker Desktop: https://www.docker.com/products/docker-desktop Open docker (e.g.: if it\u2019s running, it may show an icon in the top menu bar). In the terminal (or equivalent): docker pull busybox //pulls the image named \"busybox\" from DockerHub docker run busybox //creates the container, then exits if no tasks are running docker run busybox ls //creates container, lists files, exits docker image ls //lists all docker images docker --help // docker help","title":"The basics"},{"location":"modules/docker/#getting-files-in-and-out-of-containers","text":"We will make a folder that can get share files with our containers. In your home directory (/Users/yourname/): mkdir workspace cd workspace docker run -v /Users/[yourname]/workspace:/data/ busybox touch data/plantfile -v connectes our local \u201cworkspace\u201d folder with a \u201cdata\u201d folder in the container. Around this, we docker run busybox Finally, we create a file with touch data/plantfile","title":"Getting files in and out of containers"},{"location":"modules/docker/#example-assemble-and-annotate-a-bacterial-genome","text":"Get some sample data files (synthetic bacterial illumina reads): wget https://zenodo.org/record/582600/files/mutant_R1.fastq?download=1 wget https://zenodo.org/record/582600/files/mutant_R2.fastq?download=1 Shorten the names: mv mutant_R1.fastq?download=1 mutant_R1.fastq mv mutant_R2.fastq?download=1 mutant_R2.fastq Run the assembly tool (spades) in a container: docker run -v /Users/[yourname]/workspace:/data/ quay.io/biocontainers/spades:3.13.0--0 ./usr/local/bin/spades.py -1 data/mutant_R1.fastq -2 data/mutant_R2.fastq -o data/spades_outdir quay.io/biocontainers/spades:3.13.0\u20130 is the image name ./usr/local/bin/spades.py is the command to run spades -1 specifies the file with forward reads, data/mutant_R1.fastq -2 specifies the file with reverse reads, data/mutant_R2.fastq -o specifies the output directory, data/spades_outdir ls //to see the output files in your local workspace folder Move the assembled reads - the contigs file - into the workspace folder: mv spades_outdir/contigs.fasta . Run the annotation tool (prokka) in a container: docker run -v /Users/[yourname]/workspace:/data/ ummidock/prokka:1.13.3-01 prokka contigs.fasta ummidock/prokka:1.13.3-01 is the image name prokka is the command to run prokka the input file is contigs.fasta this container has been configured to start within the data folder, so \u201cdata/\u201d is not needed before the input file this container has also been configured to put output in the data folder, so it is not necessary to specify an output directory name ls //to see the annotation output cd PROKKA_outfile_name less PROKKA_outfile.txt //summary of annotation","title":"Example: assemble and annotate a bacterial genome"},{"location":"modules/docker/#troubleshooting","text":"How do we know what the command is to run the tool? Run the container interactively: e.g. docker run -it ummidock/prokka:1.13.3-01 Try the tool name and the help flag: e.g. prokka -h How do we know if the container has a data folder? Run the container, print the working directory: e.g. docker run ummidock/prokka:1.13.3-01 pwd If the working directory is called \u201cworkdir\u201d, use that in place of \u201cdata\u201d e.g. -v /Users/[yourname]/workspace:/workdir/","title":"Troubleshooting"},{"location":"modules/docker/#links","text":"Docker: https://docs.docker.com/get-started/","title":"Links"},{"location":"modules/envs/","text":"Set up a vitual environment A virtual environment is an area to work in that is associated with certain tools and versions. It is often useful to work on different bioinformatics analyses or tasks in different virtual environments, as they may rely on particular versions of tools. One way to set this up is to use a package manager called conda. Install miniconda: https://docs.conda.io/en/latest/miniconda.html In the terminal (or equivalent), set up the bioconda channel: (More information: https://bioconda.github.io/ ) conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge This example shows how to set up and use an environment: conda create --name mrbayes //create new environment called mrbayes conda activate mrbayes //activate this environment conda install mrbayes //install the \"mrbayes\" tool [amazing analysis] //run your analysis conda deactivate //deactive this environment Search here for tools: https://anaconda.org/bioconda/ Using conda conda env list //what environments have you set up? conda list -n mrbayes // what's installed in the mrbayes environment","title":"Virtual environments"},{"location":"modules/envs/#set-up-a-vitual-environment","text":"A virtual environment is an area to work in that is associated with certain tools and versions. It is often useful to work on different bioinformatics analyses or tasks in different virtual environments, as they may rely on particular versions of tools. One way to set this up is to use a package manager called conda. Install miniconda: https://docs.conda.io/en/latest/miniconda.html In the terminal (or equivalent), set up the bioconda channel: (More information: https://bioconda.github.io/ ) conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge This example shows how to set up and use an environment: conda create --name mrbayes //create new environment called mrbayes conda activate mrbayes //activate this environment conda install mrbayes //install the \"mrbayes\" tool [amazing analysis] //run your analysis conda deactivate //deactive this environment Search here for tools: https://anaconda.org/bioconda/","title":"Set up a vitual environment"},{"location":"modules/envs/#using-conda","text":"conda env list //what environments have you set up? conda list -n mrbayes // what's installed in the mrbayes environment","title":"Using conda"},{"location":"modules/help/","text":"BioStars SeqAnswers","title":"Help"},{"location":"modules/hpc/","text":"Apart from your local computer or server, you can analyse data on high-performance computers. HPC options in Australia NCI NCI is Australia\u2019s National Computational Infrastructure. You can apply for HPC access directly, or via your organisation if it is affiliated. http://nci.org.au/access/getting-access-to-the-national-facility/ Spartan at Melbourne University Melbourne University is not affiliated with NCI. Instead, you can use Spartan: https://dashboard.hpc.unimelb.edu.au/ Using HPC systems Introduction to HPC https://www.melbournebioinformatics.org.au/tutorials/tutorials/hpc/hpc/ How to use Spartan Write a slurm script \u201cSlurm\u201d schedules all the jobs on the computer. The slurm script tells the computer what to run. How to write a script https://dashboard.hpc.unimelb.edu.au/getting_started/ Generate a site_description Check which software is installed A very basic slurm script","title":"Using HPC"},{"location":"modules/hpc/#hpc-options-in-australia","text":"NCI NCI is Australia\u2019s National Computational Infrastructure. You can apply for HPC access directly, or via your organisation if it is affiliated. http://nci.org.au/access/getting-access-to-the-national-facility/ Spartan at Melbourne University Melbourne University is not affiliated with NCI. Instead, you can use Spartan: https://dashboard.hpc.unimelb.edu.au/","title":"HPC options in Australia"},{"location":"modules/hpc/#using-hpc-systems","text":"Introduction to HPC https://www.melbournebioinformatics.org.au/tutorials/tutorials/hpc/hpc/ How to use Spartan Write a slurm script \u201cSlurm\u201d schedules all the jobs on the computer. The slurm script tells the computer what to run. How to write a script https://dashboard.hpc.unimelb.edu.au/getting_started/ Generate a site_description Check which software is installed A very basic slurm script","title":"Using HPC systems"},{"location":"modules/nextflow/","text":"Nextflow The basics Install nextflow Write a script to run a workflow Nextflow documentation is good - prob delete this below A nextflow script Make a file called weather.nf nano weather.nf Add this to the script: #!/usr/bin process weather { echo true \"\"\" echo \"stormy\" \"\"\" } process is a step in the workflow echo \u201cstormy\u201d is the code block echo true allows it to print to screen the process is enclosed in these: { } Run the workflow: nextflow run weather.nf The screen will show: N E X T F L O W ~ version 19.01.0 Launching `weather.nf` [stupefied_legentil] - revision: eccaf7e744 [warm up] executor > local [4a/bb33c4] Submitted process > weather stormy Our output is on the last line Although we printed the output to the screen, nextflow has also saved the output in a default work directory See [4a/bb33c4] next to Submitted process - this is the name of the exectued process called \u201cweather\u201d. This is also a folder of output in your work directory. cd 4a/bb33c4 (replace with your process number) ls -la to see hidden files (these start with a . ) less .command.out to look at output Inputs, outputs, channels Example script: #!/usr/bin/env nextflow //env is a command that looks in the user's path for nextflow //this will use whatever version of nextflow is first in the path //input file params.in = \"reads.fasta\" //a channel called \"sequences\" sequences = file(params.in) process split { input: file 'zebra.fa' from sequences output: file 'part_zeb.fa' into records //\"records\" will be a new channel \"\"\" head -10 zebra.fa > part_zeb.fa //note any programs in here need to be installed in the environment \"\"\" } process show { input: file x from records //\"records\" is a channel output: stdout result \"\"\" less $x //refer to variable x \"\"\" } result.subscribe { println it } Links https://nf-co.re/nextflow_tutorial https://github.com/nextflow-io/awesome-nextflow","title":"Workflows"},{"location":"modules/nextflow/#nextflow","text":"","title":"Nextflow"},{"location":"modules/nextflow/#the-basics","text":"Install nextflow Write a script to run a workflow Nextflow documentation is good - prob delete this below","title":"The basics"},{"location":"modules/nextflow/#a-nextflow-script","text":"Make a file called weather.nf nano weather.nf Add this to the script: #!/usr/bin process weather { echo true \"\"\" echo \"stormy\" \"\"\" } process is a step in the workflow echo \u201cstormy\u201d is the code block echo true allows it to print to screen the process is enclosed in these: { } Run the workflow: nextflow run weather.nf The screen will show: N E X T F L O W ~ version 19.01.0 Launching `weather.nf` [stupefied_legentil] - revision: eccaf7e744 [warm up] executor > local [4a/bb33c4] Submitted process > weather stormy Our output is on the last line Although we printed the output to the screen, nextflow has also saved the output in a default work directory See [4a/bb33c4] next to Submitted process - this is the name of the exectued process called \u201cweather\u201d. This is also a folder of output in your work directory. cd 4a/bb33c4 (replace with your process number) ls -la to see hidden files (these start with a . ) less .command.out to look at output","title":"A nextflow script"},{"location":"modules/nextflow/#inputs-outputs-channels","text":"Example script: #!/usr/bin/env nextflow //env is a command that looks in the user's path for nextflow //this will use whatever version of nextflow is first in the path //input file params.in = \"reads.fasta\" //a channel called \"sequences\" sequences = file(params.in) process split { input: file 'zebra.fa' from sequences output: file 'part_zeb.fa' into records //\"records\" will be a new channel \"\"\" head -10 zebra.fa > part_zeb.fa //note any programs in here need to be installed in the environment \"\"\" } process show { input: file x from records //\"records\" is a channel output: stdout result \"\"\" less $x //refer to variable x \"\"\" } result.subscribe { println it }","title":"Inputs, outputs, channels"},{"location":"modules/nextflow/#links","text":"https://nf-co.re/nextflow_tutorial https://github.com/nextflow-io/awesome-nextflow","title":"Links"},{"location":"modules/platforms/","text":"Galaxy Galaxy is a web platform for bioinformatics. There are many public Galaxy servers. Australia\u2019s Galaxy server is https://usegalaxy.org.au/ Training for Galaxy Australia: https://galaxy-au-training.github.io/tutorials/ Galaxy Training Network (for various Galaxy servers): https://galaxyproject.github.io/training-material/ CyVerse","title":"Platforms"},{"location":"modules/platforms/#galaxy","text":"Galaxy is a web platform for bioinformatics. There are many public Galaxy servers. Australia\u2019s Galaxy server is https://usegalaxy.org.au/ Training for Galaxy Australia: https://galaxy-au-training.github.io/tutorials/ Galaxy Training Network (for various Galaxy servers): https://galaxyproject.github.io/training-material/","title":"Galaxy"},{"location":"modules/platforms/#cyverse","text":"","title":"CyVerse"},{"location":"modules/start/","text":"Under development: 2019 Introduction What is plant genomics What is bioinformatics Getting started in plant genomics Where to start Useful papers Useful links Useful databases and resources Where to get help Biostars SeqAnswers StackOverflow Tutorials Data Carpentry Genomics Workshop: https://datacarpentry.org/lessons/#genomics-workshop ISU Genomics: https://isugenomics.github.io/bioinformatics-workbook/ Sequence analysis workshop UCDavis: https://angus.readthedocs.io/en/2018/ Melbourne Bioinformatics: https://www.melbournebioinformatics.org.au/training-and-events/ Test yourself Rosalind bioinformatics problems: http://rosalind.info/problems/locations/","title":"Getting started"},{"location":"modules/start/#introduction","text":"What is plant genomics What is bioinformatics","title":"Introduction"},{"location":"modules/start/#getting-started-in-plant-genomics","text":"Where to start Useful papers Useful links Useful databases and resources","title":"Getting started in plant genomics"},{"location":"modules/start/#where-to-get-help","text":"Biostars SeqAnswers StackOverflow","title":"Where to get help"},{"location":"modules/start/#tutorials","text":"Data Carpentry Genomics Workshop: https://datacarpentry.org/lessons/#genomics-workshop ISU Genomics: https://isugenomics.github.io/bioinformatics-workbook/ Sequence analysis workshop UCDavis: https://angus.readthedocs.io/en/2018/ Melbourne Bioinformatics: https://www.melbournebioinformatics.org.au/training-and-events/","title":"Tutorials"},{"location":"modules/start/#test-yourself","text":"Rosalind bioinformatics problems: http://rosalind.info/problems/locations/","title":"Test yourself"},{"location":"modules/tools/","text":"General tools BLAST Sequence matching Paper: A Guide to BLAST: https://f1000research.com/documents/7-1435 Tutorial: BLAST: https://isugenomics.github.io/bioinformatics-workbook/dataAnalysis/blast/blast_index Genome assembly Canu For noisy long-read assemblies. Github: https://github.com/marbl/canu Manual: https://canu.readthedocs.io/en/latest/ wtdbg2 For noisy long-read assemblies. Does not error-correct the reads: fast; uses \u201cfuzzy\u201d de Bruijn graphs. Github: https://github.com/ruanjue/wtdbg2 Masurca Joins short reads into super-reads; for hybrid assemblies Github: https://github.com/alekseyzimin/masurca/releases Blog: http://masurca.blogspot.com/ Flye For noisy long-read assemblies Does not error-correct the reads: fast. Github: https://github.com/fenderglass/Flye Supernova For assembling 10x (linked) reads Information: https://support.10xgenomics.com/de-novo-assembly/software/pipelines/latest/using/running miniasm https://github.com/lh3/miniasm Racon Pilon Nanopolish","title":"Tools"},{"location":"modules/tools/#general-tools","text":"BLAST Sequence matching Paper: A Guide to BLAST: https://f1000research.com/documents/7-1435 Tutorial: BLAST: https://isugenomics.github.io/bioinformatics-workbook/dataAnalysis/blast/blast_index","title":"General tools"},{"location":"modules/tools/#genome-assembly","text":"Canu For noisy long-read assemblies. Github: https://github.com/marbl/canu Manual: https://canu.readthedocs.io/en/latest/ wtdbg2 For noisy long-read assemblies. Does not error-correct the reads: fast; uses \u201cfuzzy\u201d de Bruijn graphs. Github: https://github.com/ruanjue/wtdbg2 Masurca Joins short reads into super-reads; for hybrid assemblies Github: https://github.com/alekseyzimin/masurca/releases Blog: http://masurca.blogspot.com/ Flye For noisy long-read assemblies Does not error-correct the reads: fast. Github: https://github.com/fenderglass/Flye Supernova For assembling 10x (linked) reads Information: https://support.10xgenomics.com/de-novo-assembly/software/pipelines/latest/using/running miniasm https://github.com/lh3/miniasm Racon Pilon Nanopolish","title":"Genome assembly"}]}